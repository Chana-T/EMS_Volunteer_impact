{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2/2: EDA & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "* [Data import & initial EDA](#Data-import-&-initial-EDA)\n",
    "* [Aggregate visualizations](#Aggregate-visualizations)\n",
    "    * [Canceled calls](#Canceled-calls)\n",
    "    * [Severity level codes](#Severity-level-codes)\n",
    "* [Mapping dispatch activity by zipcode](#Mapping-dispatch-activity-by-zipcode)\n",
    "* [K-Modes Clustering...](#K-Modes-Clustering...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import scipy\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 35)\n",
    "pd.set_option('display.max_rows', 110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import & initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context:\n",
    "Many volunteer EMS companies in NYC identify emergency calls by listening in on the public 911 dispatches and respond to those calls alongside the 911 units. The 911 units are frequently working 12-hr shifts, often with mandatory overtime, and don't necessarily mind handing the patient off to the volunteer units. When this happens, the disposition code referring to the call is given as an 87 (call canceled) or 94 (treated & transferred care). At the same time, not all canceled calls are canceled because of the presence of volunteer units. This notebook will focus on the canceled calls (87 & 94) to determine whether volunteer EMS has a detectable influence on NYC's EMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data as Pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in processed csv file as Pandas df--data not stored in GitHub repository due to large file size\n",
    "df_concat = pd.read_csv('~/Documents/GA_2019/Projects/EMS_dispatch_evenings.csv', \n",
    "                        parse_dates = ['INCIDENT_DATETIME',\n",
    "                                      'FIRST_ASSIGNMENT_DATETIME',\n",
    "                                      'FIRST_ACTIVATION_DATETIME',\n",
    "                                      'FIRST_ON_SCENE_DATETIME',\n",
    "                                      'FIRST_TO_HOSP_DATETIME',\n",
    "                                      'FIRST_HOSP_ARRIVAL_DATETIME',\n",
    "                                      'INCIDENT_CLOSE_DATETIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When the full dataset is being processed, all date-time columns are converted from object type to datetime type. But once the resulting dataframe is saved to .csv, this formatting is lost, because spreadsheet-format doesn't preserve data types. Therefore the 'parse_dates' in the above pd.read_csv is required, it reminds the dates of the format they're supposed to be in. The initial function from Notebook 1 was still necessary as a first step because it formatted the dates so that parse_dates could do its thing without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How does our data look?\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is this dataset?\n",
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_concat.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "* The variables are almost completely categorical, so df.describe() produces pretty meaningless results.\n",
    "* On the other hand, df.describe for object types shows unique and most frequently occurring values--the top call type is SICK, and the borough with the greatest number of calls is Brooklyn, while the dispatch area with the most calls is in the south Bronx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.describe(include = ['O']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check dtypes\n",
    "df_concat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert INCIDENT_DISPOSITION_CODE and ZIPCODE to int type to reduce the amount of memory they use\n",
    "df_concat[['INCIDENT_DISPOSITION_CODE','ZIPCODE']] = df_concat[['INCIDENT_DISPOSITION_CODE','ZIPCODE']].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes again--now everything is as it should be\n",
    "df_concat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show INCIDENT_DISPOSITION_CODE distribution\n",
    "df_concat['INCIDENT_DISPOSITION_CODE'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize INCIDENT_DISPOSITION_CODE distribution\n",
    "df_concat['INCIDENT_DISPOSITION_CODE'].value_counts(normalize = True).plot.bar();\n",
    "plt.xlabel('INCIDENT_DISPOSITION_CODE');\n",
    "plt.ylabel('Fraction of total calls');\n",
    "plt.title('INCIDENT_DISPOSITION_CODE distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Incident disposition codes--\n",
    "* 82: Transporting patient (to hospital)\n",
    "* 83: Patient pronounced dead\n",
    "* 87: Call canceled\n",
    "* 90: Call unfounded/no patient found\n",
    "* 91: Condition corrected\n",
    "* 92: Treated not transported\n",
    "* 93: Refused medical attention\n",
    "* 94: Treated & transferred care\n",
    "* 95: Triaged at scene no transport\n",
    "* 96: Patient gone upon arrival at scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The incident disposition codes of interest to us are 87 & 94: 87 refers to calls canceled, generally by other units, while 94 refers to calls where the unit provided some treatment on-scene, but the patient was transported by another unit.\n",
    "* 87 = 3.26% of calls\n",
    "* 94 = .09% of calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show INITIAL_CALL_TYPE distribution\n",
    "df_concat['INITIAL_CALL_TYPE'].value_counts(normalize = True).plot.bar(figsize = (25, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are over 100 possible call types that appear in this dataset, but their frequencies aren't evenly distributed--a handful of call types cover the vast majority of calls. Each call type is generally associated with 1 of 9 possible severity levels, with a level of 1 being most severe and 7 being least severe. 8 & 9 are infrequently used and usually refer specifically to the `STNDBY` calls, for which an ambulance is requested to stand-by in case it is needed, such as at the NY Marathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show INITIAL_SEVERITY_LEVEL_CODE distribution\n",
    "df_concat['INITIAL_SEVERITY_LEVEL_CODE'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 64937 total 87+94s in our dataset\n",
    "df_concat[(df_concat['INCIDENT_DISPOSITION_CODE'] == 87) ^ (df_concat['INCIDENT_DISPOSITION_CODE'] == 94)]['FIRST_ON_SCENE_DATETIME'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 64_937 total canceled calls in our data (87 & 94 combined). However, of those, roughly 1/3 never make it to the scene. Those most likely refer to calls that were canceled en-route/by dispatch, because an extra unit was sent by mistake, because the location is bad, because the unit was no longer needed, or myriad other reasons. Of those 64_937 canceled calls, 43_520 make it as far as arriving on-scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many calls where a unit was actually dispatched and canceled on-scene rather than ahead of time?\n",
    "df_concat[(df_concat['INCIDENT_DISPOSITION_CODE'] == 87) ^ (df_concat['INCIDENT_DISPOSITION_CODE'] == 94)]['FIRST_ON_SCENE_DATETIME'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe, just for canceled calls\n",
    "df_canceled = df_concat[(df_concat['INCIDENT_DISPOSITION_CODE'] == 87) ^ (df_concat['INCIDENT_DISPOSITION_CODE'] == 94)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating out all of the NOT null values for on-scene arrival, etc\n",
    "df_canceled = df_canceled[df_canceled['FIRST_ON_SCENE_DATETIME'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canceled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the time spent on scene tell us anything useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable CALL_LENGTH to represent the amount of time spent on-scene at a canceled call (in seconds)\n",
    "df_canceled['CALL_LENGTH'] = (df_canceled['INCIDENT_CLOSE_DATETIME'] - df_canceled['FIRST_ON_SCENE_DATETIME']).astype('timedelta64[s]')\n",
    "\n",
    "# Now convert this to minutes for a more intuitive number\n",
    "df_canceled['CALL_LENGTH'] = round((df_canceled['CALL_LENGTH'] / 60), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it look?\n",
    "df_canceled['CALL_LENGTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max and min call lengths (minutes)\n",
    "print(f\"Maximum call length = {np.max(df_canceled['CALL_LENGTH'])}\")\n",
    "print(f\"Minimum call length = {np.min(df_canceled['CALL_LENGTH'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "* 348 minutes = ~5.8 _hours_. A canceled call of that length can frequently be attributed to either a bug in the CAD system that wasn't corrected for, or some major incident with which the assigned unit was assisting, but did not actually end up transporting or even directly dealing with any patients themselves. (Multi-car pileups that end up having extra units at the scene just in case and/or because it's hard to tell how bad it is initially, etc.)\n",
    "* 0 minutes frequently means that a unit is already on-scene when the assigned unit arrives, and if the scene appears to be under control, they may call it in as canceled and just keep driving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many calls result in over an hour spent at the scene before they are canceled?\n",
    "df_canceled[df_canceled['CALL_LENGTH'] > 60]['INCIDENT_DISPOSITION_CODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What does the distribution of the severity levels look like, for canceled jobs?\n",
    "df_canceled['INITIAL_SEVERITY_LEVEL_CODE'].value_counts(normalize = True).sort_index().plot.bar();\n",
    "plt.xlabel('INITIAL_SEVERITY_LEVEL_CODE');\n",
    "plt.ylabel('Fraction of total calls');\n",
    "plt.title('INITIAL_SEVERITY_LEVEL_CODE distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the call type(s) that have a severity level of 8\n",
    "df_canceled[df_canceled['INITIAL_SEVERITY_LEVEL_CODE'] == 8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Standby events removed from the dataset for purposes of the histograms because they aren't incidents where volunteer units would be picking up the calls as well. Also, a respectable number of the longer call lengths are associated with standby events--possibly because a CAD # might be assigned at the start of the event and not canceled 'til after it's over. Additionally, all call lengths over an hour were removed as well, under the assumption that for the most part, they're some form of operator error (bugs in the system), or assisting other units at an emergency scene (think car crash with multiple injured patients) and not officially getting \"canceled\" until they leave the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stand-by events\n",
    "df_canceled_ns = df_canceled[df_canceled['INITIAL_CALL_TYPE'] != 'STNDBY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove events with on-scene times over 1 hour\n",
    "df_canceled_nl = df_canceled_ns[df_canceled_ns['CALL_LENGTH'] <= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How many calls have 0 time spent on-scene?\n",
    "df_canceled_nl[df_canceled_nl['CALL_LENGTH'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many calls have > 60 mins spent on-scene?\n",
    "df_canceled_ns[df_canceled_ns['CALL_LENGTH'] > 60].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a column in df_canceled_nl showing log-scale of call length\n",
    "df_canceled['LOG_LENGTH'] = np.log(df_canceled['CALL_LENGTH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For the log plot, ln(0) = $-\\infty$, so zero-values need to be cut out for the data to plot properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of time spent on-scene for canceled calls\n",
    "df_canceled_ns['CALL_LENGTH'].plot.hist(bins = 40, figsize = (12, 10));\n",
    "plt.title('Distribution of Time Spent On-Scene for Canceled Calls', fontsize = 20);\n",
    "plt.xlabel('Minutes Spent On-Scene', fontsize = 16);\n",
    "plt.ylabel('Frequency of Occurrence', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** On-scene time is heavily skewed to the right, it's hard to determine anything from this plot because of the extreme tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of time spent on-scene for canceled calls\n",
    "df_canceled_nl['CALL_LENGTH'].plot.hist(bins = 40, figsize = (12, 10));\n",
    "plt.title('Distribution of Time Spent On-Scene for Canceled Calls (Truncated)', fontsize = 20);\n",
    "plt.xlabel('Minutes Spent On-Scene', fontsize = 16);\n",
    "plt.ylabel('Frequency of Occurrence', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** With the ~600 calls with on-scene time > 1 hour removed from our dataset, the severe skew is still apparently, but the drop-off appears much less steep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of log-time spent on-scene for canceled calls\n",
    "df_canceled_ns['LOG_LENGTH'].plot.hist(bins = 40, figsize = (12, 10), range = [0,10]);\n",
    "plt.title('Distribution of Log-Time Spent On-Scene for Canceled Calls', fontsize = 20);\n",
    "plt.xlabel('Log Minutes Spent On-Scene', fontsize = 16);\n",
    "plt.ylabel('Frequency of Occurrence', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Taking the log of the on-scene time provides a more normal-looking distribution. The hope was that we would see some sort of bimodal distribution, indicating potentially different causes for the canceled calls (such as the appearance of volunteer EMS vs too many units assigned by 911, etc). Unfortunately, there seems to be no evidence of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For ease of visualization, our data here are reduced to calls in Brooklyn _only_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_bk = df_canceled[df_canceled['BOROUGH'] == 'BROOKLYN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one CAD # occurred in the 11385 zipcode, which is located primarily in Queens. \n",
    "# This incident is being dropped because it skews everything else\n",
    "df_bk = df_bk[df_bk['ZIPCODE'] != 11385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_bk,\n",
    "             vars = ['DISPATCH_RESPONSE_SECONDS_QY','INCIDENT_DISPOSITION_CODE', 'ZIPCODE', 'CALL_LENGTH'],\n",
    "             hue = 'INITIAL_SEVERITY_LEVEL_CODE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** No obvious relationships between numeric variables pop up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canceled calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "df_bk.groupby('ZIPCODE')['CAD_INCIDENT_ID'].count().plot.bar(ax = ax);\n",
    "ax.set_title('Count of Canceled Calls per Zipcode', fontsize = 20);\n",
    "ax.set_xlabel('Brooklyn Zipcodes', fontsize = 16);\n",
    "ax.set_ylabel('Call Count', fontsize = 16)\n",
    "ax.tick_params(labelsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** <br>\n",
    "\n",
    "High numbers of canceled calls: <br>\n",
    "* 11234: Flatlands, contains the Flatlands VAC\n",
    "* 11215/11217: Park Slope & Prospect Heights, both within the Park Slope VAC radius\n",
    "* 11226: Flatbush, contains East Midwood VAC, as well as significant presence of Hatzolah\n",
    "* 11221: Bedford-Stuyvesant, contains the Bedford Stuyvesant VAC\n",
    "\n",
    "Low numbers of canceled calls: <br>\n",
    "* 11222: Greenpoint, no significant VAC activity of any kind in evidence.\n",
    "* 11239: East New York/Starrett City, no significant VAC activity but also relatively low population\n",
    "* 11249: Western Williamsburg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity level codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything interesting to see about the relative proportions of different severity levels throughout Brooklyn zipcodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized distribution of INITIAL_SEVERITY_LEVEL_CODE values\n",
    "df_bk_severity = ((df_bk.groupby(['ZIPCODE', 'INITIAL_SEVERITY_LEVEL_CODE']).count())/(df_bk.groupby(['ZIPCODE']).count()).drop(columns = 'INITIAL_SEVERITY_LEVEL_CODE')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bk_severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this with a pivot table\n",
    "df_bk_severity_pivot = df_bk_severity.pivot(index = 'ZIPCODE', columns = 'INITIAL_SEVERITY_LEVEL_CODE', values = 'CAD_INCIDENT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table visualization\n",
    "df_bk_severity_pivot.plot.bar(stacked=True, figsize=(12,9), cmap = 'RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A severity level of 1 indicates a potentially critical patient, while a severity level of 7 indicates a relatively low-priority call. High absolute numbers of canceled calls don't appear to relate significantly to call priority--11234, for instance, has a higher proportion of severe calls (codes 1-3), while 11215 and 11217 have lower proportions of severe calls. Call severity may well also have more to do with population demographics than anything else--certain neighborhoods will have higher numbers of the very old or the very young, with accompanying respiratory vulnerability, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping dispatch activity by zipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a mappable dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import zipcodes shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to map the polygons to a coordinate reference system\n",
    "gdf = gpd.read_file('../data/zipcodes/ZIP_CODE_040114.shp').to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What do our zipcodes look like?\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gdf zipcode column to same datatype as df_concat zipcode column\n",
    "gdf['ZIPCODE'] = gdf['ZIPCODE'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the one fully duplicate row...though this doesn't deal with the other partial duplicates\n",
    "gdf.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of rows with duplicate zipcodes\n",
    "dupzips_gdf = gdf.loc[gdf.duplicated(subset = 'ZIPCODE')].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A number of rows are partial duplicates--they contain duplicate zipcodes, with different areas and different polygon boundaries. Sometimes these differences are slight, sometimes they are more extreme. In our case, however, polygon boundaries are only for mapping purposes, not for calculating anything qualitatively--the only _values_ we need from the shapefile are zipcode and population, which are consistent across the duplicates. Additionally, it seemed impossible to independently verify _which_ zipcode area was the correct one--numbers available through Google did not match any of the values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop partial duplicates\n",
    "gdf.drop(index = dupzips_gdf, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipcodes in dispatch df & NOT in zipcodes df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many zipcodes in dispatch dataset?\n",
    "set_zip_df = set(list(df_concat['ZIPCODE'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set_zip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many zipcodes in zipcodes gdf?\n",
    "set_zip_gdf = set(list(gdf['ZIPCODE'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set_zip_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of rows with anomalous zipcodes\n",
    "badzip_ind = df_concat.loc[df_concat.ZIPCODE.isin(list(set_zip_df - set_zip_gdf))].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badzip_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** No zipcodes in df that aren't in the zipcodes df, which is helpful/means less manipulation is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipcodes in zipcodes df & NOT in dispatch df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These zipcodes will be eliminated when this df is joined to the dispatch df on the zip column\n",
    "len(set_zip_gdf - set_zip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list of zipcodes in gdf and not df\n",
    "zips_unshared = list(set_zip_gdf - set_zip_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show list of zipcodes\n",
    "zips_unshared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show rows with zipcodes not found in the dispatch dataset\n",
    "gdf[gdf.ZIPCODE.isin(zips_unshared)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the distribution of INCIDENT_DISTRIBUTION_CODE per zipcode\n",
    "(df_concat.groupby(['ZIPCODE', 'INCIDENT_DISPOSITION_CODE']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And now, normalized INCIDENT_DISPOSITION_CODE distribution per zip\n",
    "df_normzipdisp = ((df_concat.groupby(['ZIPCODE', 'INCIDENT_DISPOSITION_CODE']).count())/(df_concat.groupby(['ZIPCODE']).count())).drop(columns = 'INCIDENT_DISPOSITION_CODE').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normzipdisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aaaaand merge the dataframes, at long last!\n",
    "merged_df = pd.merge(df_normzipdisp, gdf, how = 'left', on = 'ZIPCODE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Any time data needs to be mapped, make sure the data is in the exact shape/format it needs for plotting _before_ it is merged with the geometry data. Otherwise any aggregation techniques used will be applied to the geometry as well, which will kill its ability to create lovely maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GeoPandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoPandas DataFrame from merged df and its associated geometry column\n",
    "gdf_geo = gpd.GeoDataFrame(merged_df, geometry = merged_df.geometry, \n",
    "                            crs = {'init':'epsg:4326', 'no_defs':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If just plotting canceled calls, we want to look at the combination of 87 and 94 calls only, we don't care about the rest. Their individual, normalized contributions to the dataset are all in merged_df and gdf_geo, so now we isolate and combine those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_geo_87 = gdf_geo[gdf_geo['INCIDENT_DISPOSITION_CODE'] == 87]\n",
    "gdf_geo_94 = gdf_geo[gdf_geo['INCIDENT_DISPOSITION_CODE'] == 94]\n",
    "gdf_canceled_merged = pd.merge(gdf_geo_87, gdf_geo_94, how = 'outer', on = 'ZIPCODE', suffixes=('_87', '_94'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_canceled_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_canceled_merged['geometry_87'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_canceled_merged['ZIPCODE'].notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While not all zipcodes contain both 87 and 94 calls, especially the ones with low call volume overall, all zipcodes _do_ seem to contain 87 calls, so that's a column that can be used as a reference and the 94 values just added to it where they exist at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NaNs to 0\n",
    "gdf_canceled_merged['BOROUGH_94'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Where a zipcode does _not_ have any 94 calls, the merge fills those cells with NaN values instead, so we replace the NaNs with 0 so that the columns can be safely added together without issue or error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new, empty GeoDataFrame\n",
    "gdf_canceled = gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this GeoDataFrame with only the columns that we actually need\n",
    "gdf_canceled['ZIPCODE'] = gdf_canceled_merged['ZIPCODE']\n",
    "gdf_canceled['CANCELED_PROP'] = gdf_canceled_merged['BOROUGH_87'] + gdf_canceled_merged['BOROUGH_94']\n",
    "gdf_canceled['POPULATION'] = gdf_canceled_merged['POPULATION_87']\n",
    "gdf_canceled['geometry'] = gdf_canceled_merged['geometry_87']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it look?\n",
    "gdf_canceled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the canceled call ratio distributed across the city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot ratio of canceled calls per zip\n",
    "fig, ax = plt.subplots(figsize = (13, 13))\n",
    "gdf.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_canceled.plot('CANCELED_PROP', legend=True, ax=ax,  # All df columns are identical\n",
    "             cmap='viridis_r', legend_kwds={'fraction':.035}, # Scale legend height to plot\n",
    "             vmin = 0,\n",
    "             vmax = 0.15)\n",
    "ax.set_title('Ratio of Canceled Calls/Total Calls per Zipcode', fontsize = 20)\n",
    "ax.set_xlabel('Longitude', fontsize = 16)\n",
    "ax.set_ylabel('Latitude', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** There are a number of distinct zipcodes with above-normal ratios of canceled calls. These zipcodes frequently correspond with zipcodes that have higher total numbers of canceled calls (non-normalized). An area that hasn't come up in discussion so far is Staten Island--the VAC there responds to calls south of the Staten Island Expressway. Something like 75-80% of SI is south of the expressway, and interestingly, those are the areas of SI with higher canceled call ratios. Major parks like Central Park and Fort Tilden have _extremely_ high canceled call ratios, as does JFK Airport to a lesser extent, but those are probably due to unrelated factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about overall call frequency per capita?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the distribution of INCIDENT_DISTRIBUTION_CODE per zipcode\n",
    "df_zipcalls = (df_concat.groupby(['ZIPCODE']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with zipcodes df\n",
    "df_zipcalls_geo = pd.merge(df_zipcalls, gdf, how = 'left', on = 'ZIPCODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoDataFrame for mapping\n",
    "gdf_zipcalls = gpd.GeoDataFrame(df_zipcalls_geo, geometry = df_zipcalls_geo.geometry, \n",
    "                            crs = {'init':'epsg:4326', 'no_defs':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_zipcalls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Certain zipcodes have populations of 0--think of areas in Manhattan with no residential zoning. EMS will be called to those locations to treat patients, but those incidents won't be factored into a calculation of calls per capita. Out of 227 zipcodes in NYC, 41 have POPULATION = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_zippop = gdf_zipcalls[gdf_zipcalls['POPULATION'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_zippop['PERCAP'] = gdf_zippop['CAD_INCIDENT_ID'] / gdf_zippop['POPULATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identify zipcodes with >1 call per person\n",
    "gdf_zippop[gdf_zippop['PERCAP'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "* 10001: Commercial district just south of midtown-Manhattan, minimal residents, but many people working\n",
    "* 10004: Battery Park, at the southern tip of Manhattan, similar situation as 10001\n",
    "* 10018: Garment District in Manhattan, same as above\n",
    "* 11430: JFK Airport, very few residents but a lot of traffic at all times of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot calls per capita\n",
    "fig, ax = plt.subplots(figsize = (13, 13))\n",
    "gdf.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_zippop.plot('PERCAP', legend=True, ax=ax,  # All df columns are identical\n",
    "             cmap='viridis_r', legend_kwds={'fraction':.035}, # Scale legend height to plot\n",
    "             vmin = 0, vmax = 1)\n",
    "ax.set_title('Calls per Capita', fontsize = 20)\n",
    "ax.set_xlabel('Longitude', fontsize = 16)\n",
    "ax.set_ylabel('Latitude', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** This is where we introduce another factor in NYC EMS: the Hatzolah volunteers. A volunteer agency with hundreds of ambulances of its own that operates entirely outside of the 911 system--they have a private emergency number, and dispatchers of their own. They are located in many of the Jewish neighborhoods throughout NYC. In those areas, when people have medical emergencies, they frequently call Hatzolah over 911 because the response times are faster--the Hatzolah EMTs and medics are only a couple of houses over and can respond in under 2 minutes, while 911 is often around 10 minutes away. Because of this, fewer 911 ambulances are dispatched to these areas. This seems most evident in the southern part of Brooklyn, in places like Boro Park and Flatbush--note the lower number of calls per capita in the lower half of Brooklyn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot total call volume per zip\n",
    "fig, ax = plt.subplots(figsize = (13, 13))\n",
    "gdf.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_zippop.plot('CAD_INCIDENT_ID', legend=True, ax=ax,  # All df columns are identical\n",
    "             cmap='viridis_r', legend_kwds={'fraction':.035}, # Scale legend height to plot\n",
    "             vmin = 0)\n",
    "ax.set_title('Total Calls per Zipcode', fontsize = 20)\n",
    "ax.set_xlabel('Longitude', fontsize = 16)\n",
    "ax.set_ylabel('Latitude', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** The intent of this figure was mostly just to show that overall call volume and calls per capita aren't going to be too related. Populations vary wildly across zipcodes, not just because of high-rises vs houses, but because of residential vs commercial zoning and similar factors as well. Roughly 1/4 of NYC zipcodes have population of 0, but still require the presence of EMS on many occasions. JFK airport has a population of 16, because that's what the census states, but that doesn't mean it only has 16 ppl-worth of 911 calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Modes Clustering..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** k-modes was chosen as the model to use on this data because we have no y, which makes our learning unsupervised, and our data is also almost entirely categorical, and therefore unsuited to k-means clustering. Additionally, none of the evaluation metrics commonly used for k-means (silhouette score, inertia) work for k-modes because the 'distances' calculated within and between clusters would have nothing to do with actual numbers. The data is only numeric because it is dummied for the purposes of the model, but it's an artificial kind of 'numeric'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Thanks to https://stackoverflow.com/questions/45273731/binning-column-with-python-pandas\n",
    "\n",
    "# Binning CALL_LENGTH turns it into a categorical variable, and therefore usable in k-modes\n",
    "bins = [0, 5, 7.5, 10, 12.5, 15, 22.5, 30, 60, 360]\n",
    "df_canceled['CALL_LENGTH_BINNED'] = pd.cut(df_canceled['CALL_LENGTH'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is our data distributed across bins?\n",
    "df_canceled['CALL_LENGTH_BINNED'].value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all numeric/non-categorical columns\n",
    "df_canceled_nonum = df_canceled.drop(columns = ['INCIDENT_DATETIME',\n",
    "                                             'FINAL_CALL_TYPE',\n",
    "                                             'INITIAL_SEVERITY_LEVEL_CODE',\n",
    "                                             'FINAL_SEVERITY_LEVEL_CODE',\n",
    "                                             'INCIDENT_DISPOSITION_CODE',\n",
    "                                             'FIRST_ASSIGNMENT_DATETIME',\n",
    "                                             'DISPATCH_RESPONSE_SECONDS_QY',\n",
    "                                             'FIRST_ACTIVATION_DATETIME',\n",
    "                                             'FIRST_ON_SCENE_DATETIME',\n",
    "                                             'FIRST_TO_HOSP_DATETIME',\n",
    "                                             'FIRST_HOSP_ARRIVAL_DATETIME',\n",
    "                                             'INCIDENT_CLOSE_DATETIME',\n",
    "                                             'HELD_INDICATOR',\n",
    "                                             'INCIDENT_TRAVEL_TM_SECONDS_QY',\n",
    "                                             'INCIDENT_RESPONSE_SECONDS_QY',\n",
    "                                             'VALID_INCIDENT_RSPNS_TIME_INDC',\n",
    "                                             'VALID_DISPATCH_RSPNS_TIME_INDC',\n",
    "                                             'CALL_LENGTH',\n",
    "                                             'LOG_LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CAD_INCIDENT_ID as the index for this df\n",
    "df_canceled_nonum.set_index('CAD_INCIDENT_ID', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out events in Brooklyn only\n",
    "df_canceled_bk = df_canceled_nonum[df_canceled_nonum['BOROUGH'] == 'BROOKLYN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_canceled_bk, columns = ['INITIAL_CALL_TYPE',\n",
    "                                                'BOROUGH',\n",
    "                                                'INCIDENT_DISPATCH_AREA', 'ZIPCODE',\n",
    "                                                'CALL_LENGTH_BINNED'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does our dummied model df look like?\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate k-modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### n_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Credit to https://medium.com/@davidmasse8/unsupervised-learning-for-categorical-data-dd7e497033ae\n",
    "# and https://pypi.org/project/kmodes/\n",
    "\n",
    "# define the k-modes model\n",
    "km = KModes(n_clusters=3, init='Huang', n_init=11, verbose=1, random_state = 42)\n",
    "\n",
    "# fit the clusters to the skills dataframe\n",
    "clusters = km.fit_predict(X)\n",
    "# get an array of cluster modes\n",
    "kmodes = km.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "# For each cluster mode (a vector of \"1\" and \"0\")\n",
    "# find and print the column headings where \"1\" appears.\n",
    "# If no \"1\" appears, assign to \"no-features\" cluster.\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"no-features cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in X.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new column in the Brooklyn dataframe\n",
    "df_canceled_bk['CLUSTER3'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_canceled_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby zipcode and cluster to get number of calls per cluster per zipcode\n",
    "\n",
    "bk_cluster_agg = df_canceled_bk.groupby(['ZIPCODE','CLUSTER3']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bk_cluster_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With assistance from Noah C\n",
    "\n",
    "# We isolate the cluster in each zipcode with the maximum number of calls assigned to it\n",
    "\n",
    "list_cluster = []\n",
    "list_zip = []\n",
    "for zipcode in set(bk_cluster_agg['ZIPCODE']):\n",
    "    zip_max_calls = np.max(bk_cluster_agg[bk_cluster_agg['ZIPCODE'] == zipcode]['INITIAL_CALL_TYPE'])\n",
    "    max_cluster =  bk_cluster_agg[(bk_cluster_agg['ZIPCODE']==zipcode) & (bk_cluster_agg['INITIAL_CALL_TYPE'] == zip_max_calls)]['CLUSTER3'].values[0]\n",
    "    list_cluster.append(max_cluster)\n",
    "    list_zip.append(zipcode)\n",
    "    \n",
    "df_cluster3 = pd.DataFrame({\n",
    "    'ZIPCODE' : list_zip,\n",
    "    'CLUSTER' : list_cluster\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does our dataframe look like it should?\n",
    "df_cluster3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge my clusters with the zipcodes df\n",
    "# Aaaaand merge the dataframes, at long last!\n",
    "merged_bkclusters3 = pd.merge(df_cluster3, gdf, how = 'left', on = 'ZIPCODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_bkclusters3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoPandas DataFrame from merged df and its associated geometry column\n",
    "gdf_clusters3 = gpd.GeoDataFrame(merged_bkclusters3, geometry = merged_bkclusters3.geometry, \n",
    "                            crs = {'init':'epsg:4326', 'no_defs':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot most frequently appearing cluster label per zip\n",
    "\n",
    "color3 = ['brown', 'deepskyblue', 'gold']\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (13, 13))\n",
    "gdf[gdf['COUNTY'] == 'Kings'].plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_clusters3.plot('CLUSTER', legend = True, ax=ax,  # All df columns are identical\n",
    "             cmap = mcolors.ListedColormap(color3), legend_kwds={'fraction':.035}, # Scale legend height to plot\n",
    "             vmin = 0, vmax = 2)\n",
    "\n",
    "                  \n",
    "ax.set_title('Most Popular Cluster per Zipcode, 3 Clusters', fontsize = 20)\n",
    "ax.set_xlabel('Longitude', fontsize = 16)\n",
    "ax.set_ylabel('Latitude', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** k-modes assigns a cluster to each data point. All data points in a zipcode do not necessarily appear in the same cluster. In fact, a single zipcode will usually have data points assigned to a number of different clusters. This map shows the most frequently appearing cluster for a given zipcode. For n=3 clusters, there seems to be very little differentiation within the borough of Brooklyn, since most zipcodes have the most incidents assigned to `cluster 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Credit to https://medium.com/@davidmasse8/unsupervised-learning-for-categorical-data-dd7e497033ae\n",
    "# and https://pypi.org/project/kmodes/\n",
    "\n",
    "# define the k-modes model\n",
    "km = KModes(n_clusters=10, init='Huang', n_init=11, verbose=1, random_state = 42)\n",
    "\n",
    "# fit the clusters to the skills dataframe\n",
    "clusters = km.fit_predict(X)\n",
    "# get an array of cluster modes\n",
    "kmodes = km.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "# For each cluster mode (a vector of \"1\" and \"0\")\n",
    "# find and print the column headings where \"1\" appears.\n",
    "# If no \"1\" appears, assign to \"no-skills\" cluster.\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"no-skills cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in X.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new column in the \n",
    "df_canceled_bk['CLUSTER10'] = km.labels_\n",
    "\n",
    "bk_cluster_agg = df_canceled_bk.groupby(['ZIPCODE','CLUSTER10']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With assistance from Noah C\n",
    "\n",
    "list_cluster = []\n",
    "list_zip = []\n",
    "for zipcode in set(bk_cluster_agg['ZIPCODE']):\n",
    "    zip_max_calls = np.max(bk_cluster_agg[bk_cluster_agg['ZIPCODE'] == zipcode]['INITIAL_CALL_TYPE'])\n",
    "    max_cluster =  bk_cluster_agg[(bk_cluster_agg['ZIPCODE']==zipcode) & (bk_cluster_agg['INITIAL_CALL_TYPE'] == zip_max_calls)]['CLUSTER10'].values[0]\n",
    "    list_cluster.append(max_cluster)\n",
    "    list_zip.append(zipcode)\n",
    "    \n",
    "df_cluster10 = pd.DataFrame({\n",
    "    'ZIPCODE' : list_zip,\n",
    "    'CLUSTER' : list_cluster\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge my clusters with the zipcodes df\n",
    "# Aaaaand merge the dataframes, at long last!\n",
    "merged_bkclusters10 = pd.merge(df_cluster10, gdf, how = 'left', on = 'ZIPCODE')\n",
    "\n",
    "# Create GeoPandas DataFrame from merged df and its associated geometry column\n",
    "gdf_clusters10 = gpd.GeoDataFrame(merged_bkclusters10, geometry = merged_bkclusters10.geometry, \n",
    "                            crs = {'init':'epsg:4326', 'no_defs':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot ratio of canceled calls per zip\n",
    "fig, ax = plt.subplots(figsize = (13, 13))\n",
    "gdf[gdf['COUNTY'] == 'Kings'].plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_clusters10.plot('CLUSTER10', legend=True, ax=ax,  # All df columns are identical\n",
    "             cmap = 'tab10', legend_kwds={'fraction':.035}, # Scale legend height to plot\n",
    "             vmin = 0, vmax = 9)\n",
    "                  \n",
    "ax.set_title('Most Popular Cluster per Zipcode, 10 Clusters', fontsize = 20)\n",
    "ax.set_xlabel('Longitude', fontsize = 16)\n",
    "ax.set_ylabel('Latitude', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** For `n_clusters = 10`, we get a map that looks most like cluster have been assigned based on the 7 dispatch areas (geographic bounds unknown). Distribution seems fairly even--it's definitely not actually showing 10 different clusters, but that's okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_clusters = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the k-modes model\n",
    "km = KModes(n_clusters=12, init='Huang', n_init=11, verbose=1, random_state = 42)\n",
    "\n",
    "# fit the clusters to the skills dataframe\n",
    "clusters = km.fit_predict(X)\n",
    "\n",
    "# get an array of cluster modes\n",
    "kmodes = km.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "\n",
    "# For each cluster mode (a vector of \"1\" and \"0\")\n",
    "# find and print the column headings where \"1\" appears.\n",
    "# If no \"1\" appears, assign to \"no-skills\" cluster.\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"no-skills cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in X.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new column in the \n",
    "df_canceled_bk['CLUSTER12'] = km.labels_\n",
    "\n",
    "bk_cluster_agg = df_canceled_bk.groupby(['ZIPCODE','CLUSTER12']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With assistance from Noah C\n",
    "\n",
    "list_cluster = []\n",
    "list_zip = []\n",
    "for zipcode in set(bk_cluster_agg['ZIPCODE']):\n",
    "    zip_max_calls = np.max(bk_cluster_agg[bk_cluster_agg['ZIPCODE'] == zipcode]['INITIAL_CALL_TYPE'])\n",
    "    max_cluster =  bk_cluster_agg[(bk_cluster_agg['ZIPCODE']==zipcode) & (bk_cluster_agg['INITIAL_CALL_TYPE'] == zip_max_calls)]['CLUSTER12'].values[0]\n",
    "    list_cluster.append(max_cluster)\n",
    "    list_zip.append(zipcode)\n",
    "    \n",
    "df_cluster12 = pd.DataFrame({\n",
    "    'ZIPCODE' : list_zip,\n",
    "    'CLUSTER' : list_cluster\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge my clusters with the zipcodes df\n",
    "# Aaaaand merge the dataframes, at long last!\n",
    "merged_bkclusters12 = pd.merge(df_cluster12, gdf, how = 'left', on = 'ZIPCODE')\n",
    "\n",
    "# Create GeoPandas DataFrame from merged df and its associated geometry column\n",
    "gdf_clusters12 = gpd.GeoDataFrame(merged_bkclusters12, geometry = merged_bkclusters12.geometry, \n",
    "                            crs = {'init':'epsg:4326', 'no_defs':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot most frequently appearing cluster label per zip\n",
    "\n",
    "color12 = ['#800000', '#e6194B', '#f58231', '#ffe119', '#bfef45', '#3cb44b',\n",
    "          '#42d4f4', '#4363d8', '#911eb4', '#f032e6', '#a9a9a9', '#000000']\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (13, 13))\n",
    "gdf[gdf['COUNTY'] == 'Kings'].plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf_clusters12.plot('CLUSTER', legend = True, ax=ax,  # All df columns are identical\n",
    "             cmap = mcolors.ListedColormap(color12), legend_kwds={'fraction':.035}, # Scale legend height to plot\n",
    "             vmin = 0, vmax = 11)\n",
    "\n",
    "                  \n",
    "ax.set_title('Most Popular Cluster per Zipcode, 12 Clusters', fontsize = 20)\n",
    "ax.set_xlabel('Longitude', fontsize = 16)\n",
    "ax.set_ylabel('Latitude', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** For `n_clusters = 12`, differentiation between clusters seems to decrease from what was visible from `n_clusters = 10`, so that may be reaching the point of 'more isn't necessarily better'. Still most likely divided by dispatch area more than anything else, based on how the clusters fall out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
